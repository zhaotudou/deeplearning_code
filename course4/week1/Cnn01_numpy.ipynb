{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现卷积层和池化层，包括前向传播和（可选）反向传播。\n",
    "\n",
    "符号说明：\n",
    "- $n_H$, $n_W$ and $n_C$ denote respectively the height, width and number of channels of a given layer. If you want to reference a specific layer $l$, you can also write $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ and $n_{C_{prev}}$ denote respectively the height, width and number of channels of the previous layer. If referencing a specific layer $l$, this could also be denoted $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. \n",
    "\n",
    "## 1. packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 任务梳理\n",
    "将要实现卷积神经的基本块，要实现的每个都有详细的指示。\n",
    "\n",
    "卷积层函数：\n",
    "- 0 padding\n",
    "- 卷积窗口\n",
    "- 卷积正向\n",
    "- 卷积反向\n",
    "\n",
    "池化层函数：\n",
    "- 池化正向\n",
    "- 创建面具mask\n",
    "- Distribute value\n",
    "- 反向传播\n",
    "\n",
    "这节将从头开始（from scratch）实现这些函数，并且使用tensorflow等价实现下面的模型：\n",
    "![卷积网络结构](https://upload-images.jianshu.io/upload_images/1779926-88a55eb081cb9d94.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "对应每一个正向函数，都会有对应的反向传播公式。因此，在正向传播的每一步都会存储参数parameters到一个cache中，这些参数将用于计算梯度下降在反向传播过程中。\n",
    "\n",
    "## 3. 卷积神经网络\n",
    "### 3.1 0 padding\n",
    "就是添加0到图片的边缘上的函数，实现效果如图：\n",
    "![0 padding填充函数](https://upload-images.jianshu.io/upload_images/1779926-e0fee2d7e8f77a44.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "填充的主要好处：\n",
    "1. 允许使用卷积层后没有对高和宽进行压缩，建立深度网络时候这很重要。一个重要的类别：same padding，即是在一个卷积操作过后，图片的高和宽都没有改变。\n",
    "2. 帮助我们保留更多关于图片边缘的信息，如果没有填充，图像边缘信息就会丢失。\n",
    "\n",
    "关于np.pad函数用法的csdn博客：https://blog.csdn.net/qq_36332685/article/details/78803622\n",
    "\n",
    "**Exercise**: Implement the following function, which pads all the images of a batch of examples X with zeros. [Use np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). Note if you want to pad the array \"a\" of shape $(5,5,5,5,5)$ with `pad = 1` for the 2nd dimension, `pad = 3` for the 4th dimension and `pad = 0` for the rest, you would do:\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), 'constant', constant_values = (..,..))\n",
    "```\n",
    "\n",
    "如果将一个维度为(5,5,5,5,5)表示5维的5*5*5*5*5的一个数组，要在第二维扩充一个像素，第4维上扩充3个像素就使用上面的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  1,  0,  0,  2,  2,  2],\n",
       "       [ 1,  1,  0,  0,  2,  2,  2],\n",
       "       [ 1,  1,  0,  0,  2,  2,  2],\n",
       "       [ 1,  1, 95, 96,  2,  2,  2],\n",
       "       [ 1,  1, 97, 98,  2,  2,  2],\n",
       "       [ 1,  1,  0,  0,  2,  2,  2],\n",
       "       [ 1,  1,  0,  0,  2,  2,  2]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(95,99).reshape(2,2) \n",
    "np.pad(A,((3,2),(2,3)),'constant',constant_values = ((0,0),(1,2)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 2, 2],\n",
       "       [1, 1, 1, 2, 2],\n",
       "       [3, 3, 3, 4, 4],\n",
       "       [3, 3, 3, 4, 4],\n",
       "       [3, 3, 3, 4, 4]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.arange(1,5).reshape(2,2)\n",
    "np.pad(B,((1,2),(2,1)),'edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4, 3, 4, 4],\n",
       "       [2, 2, 1, 2, 2],\n",
       "       [4, 4, 3, 4, 4],\n",
       "       [4, 4, 3, 4, 4],\n",
       "       [4, 4, 3, 4, 4]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.arange(1,5).reshape(2,2) \n",
    "np.pad(B,((1,2),(2,1)),'maximum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于np.pad(array, pad_width, mode, \\*\\*kwargs)函数，返回值是一个数组，其中参数的意思是，array传入的数组，pad_width,扩充的宽度，一般都是一个(x,y)的内容，分别表示依次维上前面扩充x个像素，后面扩充y个像素，mode表示选择扩充的模式，比如有常量扩充，自己设置扩充什么值，对应的值放在kwargs参数中，以及边缘扩充，就是边上本来是什么值，就按什么值扩充，以及最大值扩充，按那个维度上最大的值进行前后扩充。\n",
    "![如何填充](https://upload-images.jianshu.io/upload_images/1779926-961a80bcb872c4d0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "红色部分就是一维即行上，上面添3个0，下面添2个0，然后下一个维度上，前面添2个1，后面添3个2。（3,2）（2,3）说的是第一维第二维前面后面扩充几个像素，后面的(0,0)(1,2)表示对应维度是上前后分别制定填入什么常数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X,((0,0),(pad,pad),(pad,pad),(0,0)),'constant')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (4, 7, 7, 2)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xb4d0208>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUEAAACqCAYAAAAz+v3EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x946ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 卷积的single step\n",
    "这一部分，实现卷积操作的单步，就是将过滤器放到输入原始图像的一个位置上。\n",
    "- Takes an input volume \n",
    "- Applies a filter at every position of the input\n",
    "- Outputs another volume (usually of different size)\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : **Convolution operation**<br> with a filter of 2x2 and a stride of 1 (stride = amount you move the window each time you slide) </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_single_step\n",
    "# 计算一个过滤器在原始图像的某一位置时的计算结果，输入为原始图像上的窗口矩阵，过滤器也就是W组成的，b偏置，计算结果为一个实数\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice and W. Add bias.\n",
    "    s = np.multiply(a_slice_prev,W)+b\n",
    "    # Sum over all entries of the volume s\n",
    "    Z = np.sum(s)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -23.16021220252078\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 卷积神经网络-前向传播\n",
    "这里就要实现一个卷积操作了，就是一个过滤器在原始图片上的滑动，最后计算出一个二维矩阵。\n",
    "\n",
    "关于在一个（nh,nw,nc）上切割出一个跟过滤器大小相同的块的方法是：使用矩阵的切割:a_slice_prev = a_prev[0:2,0:2,:]表示在a_prev这个三维矩阵上切割2\\*2\\*nc大小的块。如果窗口移动，则修改对应的索引值。\n",
    "\n",
    "\n",
    "![切片操作如何定义索引起始位置](https://upload-images.jianshu.io/upload_images/1779926-d5dc7a37ffdae9c6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "![卷积层后输出矩阵维度计算](https://upload-images.jianshu.io/upload_images/1779926-997214381d5d2822.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: conv_forward\n",
    "import math\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = np.int(math.floor((n_H_prev - f+2*pad)/stride+1))\n",
    "    n_W = np.int(math.floor((n_W_prev - f+2*pad)/stride+1))\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "    Z = np.random.rand(m,n_H,n_W,n_C)\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev,pad)\n",
    "    \n",
    "    for i in range(m):                               # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]                               # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = stride*h\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    # 四维矩阵切片时，使用：表示该维度从头到尾，如果是个实数则表示这个维度上的第几个切片，相当于面包切片就得到二维面包片\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev,W[:,:,:,c],b[:,:,:,c])\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.15585932488906465\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 1}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，卷积层还应该包含一个激活函数，我们就直接使用激活函数计算得到：\n",
    "```python\n",
    "# Convolve the window to get back one output neuron\n",
    "Z[i, h, w, c] = ...\n",
    "# Apply activation\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```\n",
    "卷积计算后得到z，再应用激活函数。\n",
    "\n",
    "## 4.池化层\n",
    "池化层一般都减少了输入的高和宽，并且可以帮助减少计算，还有助于使特征检测器在输入中的位置更加稳定。共有两种类型的池化层：\n",
    "- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n",
    "\n",
    "- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n",
    "\n",
    "池化层没有参数需要在反向传播过程训练。然而，同样是有参数的，比如说过滤器的大小f。\n",
    "\n",
    "### 4.1 前向池化\n",
    "池化层没有扩充，计算输出的维度大小公式为：\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = stride*h\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i,vert_start:vert_end, horiz_start: horiz_end,c]\n",
    "                    #注意这里取出来一个小块的时候是针对每个通道，因为池化操作是在每一个通道上操作的，所以切片的时候的c表示每个通道\n",
    "                    #而在卷积操作中的，c代表的过滤器的个数，所以在原始图像上切片时，第4个维度全用上\n",
    "                    \n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.average(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 1.6924546  2.10025514]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 1.51981682 2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[-0.09498456  0.11180064 -0.14263511]]]\n",
      "\n",
      "\n",
      " [[[-0.09525108  0.28325018  0.33035185]]]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 1, \"f\": 4}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在已经实现了卷积网络的每一层的正向传播了，接下来部分是关于可选部分反向传播的内容。\n",
    "\n",
    "## 5. 卷积神经网络的反向传播\n",
    "在现代深度学习框架中，只需要实现正向传播，框架会自己实现反向传播。卷积神经网络的反向传播非常的复杂。如果你想自己实现，可以完成下面作业。早期课程中，实现了简单的全连接的神经网络，使用反向传播来计算代价函数关于每个参数的梯度，同样的，在卷积神经网络中，你也可以计算梯度。\n",
    "\n",
    "### 5.1 卷积层反向传播\n",
    "#### 5.1.1 计算dA\n",
    "卷积层的反向传播比较复杂，如果是简单的Z=WA+b，所以大概是dA\\*W，那么在卷积层，我们使用$W_c$表示一个过滤器，然后$dZ_{hw}$表示一个标量，对应的是代价函数关于某一卷积层z在第h行第w列上的梯度值。所以我们使用$W_c$乘以不同的dZ值，这么做来计算反向传播，相当于就是过滤器乘里的每一个窗口，将所有窗口与这个过滤器对应的乘积和计算出来，就是计算dA了。\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "# dZ[i, h, w, c]表示针对第i个样本，和第c个过滤器，所以实际上取出来就是一个切片是个二维矩阵的第h行第w列的一个实数，W[:,:,:,c]是一个三维矩阵，乘出来最后结果也是一个实数。那么也就计算出来了关于每次切下一个窗口内容后这个窗口里面的内容的导数。关于重复的那部分格子就是多次覆盖，最后得到最新值吧，纯属个人理解。\n",
    "\n",
    "#取出W的第4维的第c组，就是一个三维矩阵，将后面得到的三维数据依次填入\n",
    "```\n",
    "\n",
    "#### 5.1.2 计算dW\n",
    "关于计算W过滤器的导数，关于dw = dz*\\da\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "关于理解dA和dW，就是使用dZ里面的每个值都和对应的窗口矩阵做乘积并且取和，最后也是得到一个矩阵。\n",
    "\n",
    "#### 5.1.3 计算db\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "#关于这一行的理解，就是不管前三个维度，dZ[i, h, w, c]是一个实数，所以意思就是将第4维的第c组元素全都置为后面这个值，并且累加。那么最后其实就是把整个b都变成了跟z有关的值。例如一个二维矩阵先把第二列修改，再把第一列修改，那么最后就是把整个b都变了。\n",
    "\n",
    "#下面是举的例子，心得，对于一个多维向量，要么填入一个实数，要么填入等大小的矩阵数据\n",
    "np.random.seed(1)\n",
    "a = np.random.randn(2,2)\n",
    "b = np.random.randn(3,2)\n",
    "print(b)\n",
    "print(a)\n",
    "print(a[1,1])\n",
    "b[:,1]= a[1,1]\n",
    "print(b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape  \n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m,n_H_prev,n_W_prev,n_C_prev))                          \n",
    "    dW = np.zeros((f,f,n_C_prev,n_C))\n",
    "    db = np.zeros((1,1,1,n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]\n",
    "        da_prev_pad = dA_prev_pad[i,:,:,:]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = stride*h\n",
    "                    vert_end = vert_start+f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] +=  W[:,:,:,c] * dZ[i,h,w,c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i,h,w,c]\n",
    "                    db[:,:,:,c] += dZ[i,h,w,c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        #这一行的作用是，根据上面的代码我们计算出来的A导数都是根据padding之后的，所以我们要得到供前一层使用的dA，就应该恢复到原本的大小，把padding去掉\n",
    "        # pad：-pad加入pad是3，那么从3到-3就将前面和后面的扩充层去掉了\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad,pad:-pad,:]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 9.665267084085745\n",
      "dW_mean = 10.581741275547566\n",
      "db_mean = 76.37106919563735\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 池化层反向传播\n",
    "尽管池化层没有参数需要更新，但是仍然需要反向传播，因为前面的卷积层需要梯度下降。\n",
    "#### 5.2.1 最大池化反向传播\n",
    "在实现池化反向传播之前必须实现函数create_mask_from_window() ：\n",
    "这个函数就像一个面具，就是跟踪最大值的位置，将最大值的位置置为1，其余位置置为0。在平均池化层也很相似，不过使用不同的面具。\n",
    "\n",
    "- [np.max()]() may be helpful. It computes the maximum of an array.\n",
    "- If you have a matrix X and a scalar x: `A = (X == x)` will return a matrix A of the same size as X such that:\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```\n",
    "- Here, you don't need to consider cases where there are several maxima in a matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = (x ==np.max(x) )\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么我们要跟踪max的位置？因为这是最终影响产出的输入值，因此也就是成本。BackProp相对于成本计算梯度，因此任何影响最终的成本应该有一个非零梯度。因此，BP将“传播”的梯度回到这个特定的输入值，影响了成本。\n",
    "\n",
    "#### 5.2.2 平均池化-反向传播\n",
    "在最大池中，对于每个输入窗口，所有对输出的“影响”来自单个输入值——最大值。\n",
    "平均池，每个输入窗口的元素对输出具有同等的影响。所以实施BackProp，现在你将实现一个辅助函数将每一个位置都贡献了一样的力。\n",
    "\n",
    "For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you'll use for the backward pass will look like: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = np.float(dz)/np.float(n_H*n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones((n_H, n_W))*average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3\n",
    "把池化层的反向传播放到一起。\n",
    "You will once again use 4 for-loops (iterating over training examples, height, width, and channels). You should use an if/elif statement to see if the mode is equal to 'max' or 'average'. If it is equal to 'average' you should use the distribute_value() function you implemented above to create a matrix of the same shape as a_slice. Otherwise, the mode is equal to 'max', and you will create a mask with create_mask_from_window() and multiply it by the corresponding value of dZ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros((m,n_H_prev,n_W_prev,n_C_prev))\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev =A_prev[i,:,:,:] \n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = stride*h\n",
    "                    vert_end =  vert_start+f\n",
    "                    horiz_start = stride*w\n",
    "                    horiz_end = horiz_start+f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask,dA[i,h,w,c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i,h,w,c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f,f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev\n",
    "\n",
    "#总结，关于最大池化层的反向就是将对于原始输入图像，这次处理的哪个窗口，就将该窗口的最大值位置置为1，其余位置为0 ，因为对于最大池化层的代价计算来说\n",
    "#只有最大值发挥了作用，而其他都没有\n",
    "#关于平均层，则是使用da里面，也就是激活值，看每一个对应的窗口的平均值结果是多少，再除以窗口大小，即计算出对于原始窗口每个格子各除了多少力\n",
    "#另外，关于池化层，是没有计算z再计算a的过程的，是直接窗口移动计算得到激活值A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于这节作业，在作业之前总觉得实现起来比较复杂，但通过巧妙的命名规范可以让问题变得简单，以及对numpy多维度矩阵的使用。就比如说，在实现卷积层时，首先考虑一个过滤器在某一个位置的运算，那么就会想多个过滤器，是不是还得循环多次，然而通过将过滤器声明成4维，问题就解决了，就说明有多个过滤器。另外，关于窗口移动之后，如何知道它的位置，取出对应元素，采用的切片操作，并且通过先计算最后可能得到几次移动，再根据移动乘上步长，再去原始图像矩阵里面进行切片。当然反向传播是比较难理解的，尽量理解吧。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
