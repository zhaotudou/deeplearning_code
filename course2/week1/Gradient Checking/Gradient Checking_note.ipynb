{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本节任务是实现梯度下降确保你的反向传播实现是正确的：\n",
    "1. 实现梯度下降from scratch\n",
    "2. 理解如何使用不同的公式来检查你的反向传播实现\n",
    "3. 认识到你的反向算法应该给你一个和通过不同的公式计算出来的相同的结果。\n",
    "4. 学习如何识别那个参数的梯度值算错\n",
    "\n",
    "# 梯度检验\n",
    "你是一个致力于在全球范围内提供移动支付的团队的一员，并被要求建立一个深层的学习模型来检测欺诈——当有人付款时，你想看看付款是否有欺诈性，比如用户的账户被黑客接管了。但是反向传播是非常具有挑战性的实现，有时也有缺陷。因为这是一个关键的应用程序，你公司的CEO想确定你的反向传播的实现是正确的。你的CEO说：“给我一个证据，证明你的反向传播确实有效！”为了保证这一点，你将使用“梯度检查”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Packages\n",
    "import numpy as np\n",
    "from testCases import *\n",
    "from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1）梯度检验如何工作？反向传播计算梯度值，$\\frac{\\partial J}{\\partial \\theta}$，θ表示模型的参数。J是在正向传播过程表示损失函数的。\n",
    "因为正向传播相对容易实现，可以对你的答案感觉很自信，基于可以100%确定你计算的答案没有问题。\n",
    "\n",
    "看看导数或者梯度的定义，$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "\"$\\displaystyle \\lim_{\\varepsilon \\to 0}$\"表示极限的意思，也就是说参数很小很小。\n",
    "\n",
    "2）1维梯度检验\n",
    "考虑一个以为线性函数$J(\\theta) = \\theta x$，这个函数仅有一个参数θ，并且x作为输入。给你一个输入，你要做的就是正向传播计算代价函数J，得到J关于参数θ的函数，$J(\\theta)$，然后通过反向传播计算偏导数。梯度下降就是用来保证你的导数计算是正确的。\n",
    "![梯度检验作用](https://upload-images.jianshu.io/upload_images/5355764-87adac866e6f7c9a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "练习：给上面这个简单的函数实现正向和反向传播函数，以及关于θ参数的导数值抵用两种不同的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: forward_propagation\n",
    "\n",
    "def forward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    J -- the value of function J, computed using the formula J(theta) = theta * x\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    J = x*theta\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J = 8\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "J = forward_propagation(x, theta)\n",
    "print (\"J = \" + str(J))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#现在实现反向传播\n",
    "# GRADED FUNCTION: backward_propagation\n",
    "\n",
    "def backward_propagation(x, theta):\n",
    "    \"\"\"\n",
    "    Computes the derivative of J with respect to theta (see Figure 1).\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    \n",
    "    Returns:\n",
    "    dtheta -- the gradient of the cost with respect to theta\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    dtheta = x\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return dtheta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtheta = 2\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "dtheta = backward_propagation(x, theta)\n",
    "print (\"dtheta = \" + str(dtheta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "练习：为了证明 backward_propagation() 函数正确计算了J关于参数θ的偏导，接下来进行梯度检验。\n",
    "\n",
    "提示：首先计算通过上面的（1）公式计算导数的近似值，会用到一个很小的值ε： \n",
    "1. $\\theta^{+} = \\theta + \\varepsilon$\n",
    "2. $\\theta^{-} = \\theta - \\varepsilon$\n",
    "3. $J^{+} = J(\\theta^{+})$\n",
    "4. $J^{-} = J(\\theta^{-})$\n",
    "5.  $gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}$\n",
    "\n",
    "然后使用反向传播计算梯度值，并且把结果存储在变量grad里面，最后计算相关差值，关于gradapprox（梯度近似值）和grad之间的差值，使用下面这个公式：\n",
    "$$ difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \\tag{2}$$\n",
    "\n",
    "你需要完成以下三步：\n",
    "1. 使用np.linalg.norm(...)计算分子numerator，linalg=linear+algebra线性+代数，norm表示范数，范数是对矩阵的一个度量，是一个标量。norm(x, ord=None, axis=None, keepdims=False)   x表示要度量的向量，ord表示范数的种类。\n",
    "![关于np.linalg.norm()函数参数介绍](https://upload-images.jianshu.io/upload_images/5355764-a48d08ba5411d704.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "2. 计算分母denominator：需调用np.linalg.norm(...)两次\n",
    "3. 除法\n",
    "\n",
    "如果这个差值最后小于 $10^{-7}$,就可以很自信的说计算梯度值的结果是正确的，否则，肯定存在哪里有错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check\n",
    "\n",
    "def gradient_check(x, theta, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in Figure 1.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- a real-valued input\n",
    "    theta -- our parameter, a real number as well\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute gradapprox using left side of formula (1). epsilon is small enough, you don't need to worry about the limit.\n",
    "    ### START CODE HERE ### (approx. 5 lines)\n",
    "    thetaplus = theta + epsilon                         # Step 1\n",
    "    thetaminus = theta - epsilon                           # Step 2\n",
    "    J_plus = x* thetaplus                        # Step 3\n",
    "    J_minus = x*thetaminus                             # Step 4\n",
    "    gradapprox =(J_plus-J_minus)/(2*epsilon)                      # Step 5\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Check if gradapprox is close enough to the output of backward_propagation()\n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    grad = x\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    numerator = np.linalg.norm(grad-gradapprox)                              # Step 1'\n",
    "    denominator =np.linalg.norm(grad)+np.linalg.norm(gradapprox)                # Step 2'\n",
    "    difference =  numerator/denominator                           # Step 3'\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    if difference < 1e-7:\n",
    "        print (\"The gradient is correct!\")\n",
    "    else:\n",
    "        print (\"The gradient is wrong!\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient is correct!\n",
      "difference = 2.91933588329e-10\n"
     ]
    }
   ],
   "source": [
    "x, theta = 2, 4\n",
    "difference = gradient_check(x, theta)\n",
    "print(\"difference = \" + str(difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你的代价函数J往往都有比一维更多的输入，当你在训练神经网络的时候，θ往往都包括了 matrices $W^{[l]}$ and biases $b^{[l]}$，知道如何对一个高纬度的参数函数进行梯度检验是很重要的。\n",
    "\n",
    "## 3）N维梯度检验\n",
    "![n维代价函数的正向和反向过程](https://upload-images.jianshu.io/upload_images/5355764-850e34e89b67e576.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation_n(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation (and computes the cost) presented in Figure 3.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- training set for m examples\n",
    "    Y -- labels for m examples \n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "                    W1 -- weight matrix of shape (5, 4)\n",
    "                    b1 -- bias vector of shape (5, 1)\n",
    "                    W2 -- weight matrix of shape (3, 5)\n",
    "                    b2 -- bias vector of shape (3, 1)\n",
    "                    W3 -- weight matrix of shape (1, 3)\n",
    "                    b3 -- bias vector of shape (1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- the cost function (logistic cost for one example)\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve parameters\n",
    "    m = X.shape[1]\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "\n",
    "    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = relu(Z2)\n",
    "    Z3 = np.dot(W3, A2) + b3\n",
    "    A3 = sigmoid(Z3)\n",
    "\n",
    "    # Cost\n",
    "    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
    "    cost = 1./m * np.sum(logprobs)\n",
    "    \n",
    "    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
    "    \n",
    "    return cost, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def backward_propagation_n(X, Y, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation presented in figure 2.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input datapoint, of shape (input size, 1)\n",
    "    Y -- true \"label\"\n",
    "    cache -- cache output from forward_propagation_n()\n",
    "    \n",
    "    Returns:\n",
    "    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T)\n",
    "    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "    # 这里是故意使用一个错误的形式来验证gradient_check是否正常工作\n",
    "    #dW2 = 1./m * np.dot(dZ2, A1.T)*2\n",
    "    # 正确的形式，最后再修改的\n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T)\n",
    "    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)\n",
    "    #这里是故意使用一个错误的形式来验证gradient_check是否正常工作\n",
    "    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    #db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
    "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
    "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在执行完backward_propagation()之后，一般就得到了关于梯度值的计算结果，全部存放在了grad参数里面，但是我们不能保证这个结果是正确的，需要验证。\n",
    "\n",
    "梯度检验是如何工作的呢？\n",
    "\n",
    "公式仍然是$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "只不过θ参数不再是一个数而已，它是一个叫做“parameters”的字典，我们要实现函数\"dictionary_to_vector()\" ，这个函数可以把“parameters”字典转化为一个叫做 \"values\"的向量，方法就是reshape所有的参数\n",
    "(W1, b1, W2, b2, W3, b3)变为向量并且合并他们。\n",
    "\n",
    "函数 \"vector_to_dictionary\"就是把结果返回成字典。\n",
    "![将所有参数矩阵转化到一个向量里](https://upload-images.jianshu.io/upload_images/5355764-75f99835dcb22bd2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "练习：实现n为梯度检验函数 gradient_check_n().\n",
    "\n",
    "指示：下面是伪代码pseudo-code帮助你实现梯度检验。\n",
    "\n",
    "关于如何得到类似于一位求导，也就是一次函数求导，得到f(θ+)和f(θ-)，如果是对于多维参数，也就是多个参数，那么久把它们展开成一个列向量，把列向量的每一个元素当成一个参数，就有θ1，θ2，θ....θn然后分别修改其中一个参数，加上一个很小的epsilon，得到一个值，最后把修改n个值后得到的值也并成一个向量，那样就能得到f(θ+)，注意这里的θ表示很多参数，同理，减去一个epsilon则得到f(θ-)。\n",
    "\n",
    "伪代码：\n",
    "循环n次，共有n个元素：\n",
    "\n",
    "    设置θ向量为parameters字典展开后的列向量\n",
    "    修改向量里的第n个元素加上epsilon\n",
    "    根据修改后的向量转化为parameters字典，代入网络中计算得到代价值\n",
    "    \n",
    "    设置θ向量为parameters字典展开后的列向量\n",
    "    修改向量里的第n个元素减去epsilon\n",
    "    根据修改后的向量转化为parameters字典，代入网络中计算得到代价值\n",
    "    \n",
    "    \n",
    "  \n",
    "![n维梯度检验实现](https://upload-images.jianshu.io/upload_images/5355764-a733eb10bc90faa9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "\n",
    "def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. \n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "    \n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set-up variables\n",
    "    parameters_values,_ = dictionary_to_vector(parameters)\n",
    "    grad = gradients_to_vector(gradients)\n",
    "    num_parameters = parameters_values.shape[0]\n",
    "    J_plus = np.zeros((num_parameters, 1))\n",
    "    J_minus = np.zeros((num_parameters, 1))\n",
    "    gradapprox = np.zeros((num_parameters, 1))\n",
    "    \n",
    "    # Compute gradapprox\n",
    "    for i in range(num_parameters):\n",
    "        # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "        # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "        #_下划线的意思是我们的函数返回了两个参数，而我们只需要其中的一个，另一个不需要，所以就存在下划线这个变量里面\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaplus = np.copy(parameters_values)                                      # Step 1\n",
    "        thetaplus[i][0] =thetaplus[i][0]+epsilon                               # Step 2\n",
    "        J_plus[i], _ =  forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                         # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "        ### START CODE HERE ### (approx. 3 lines)\n",
    "        thetaminus = np.copy(parameters_values)                                     # Step 1\n",
    "        thetaminus[i][0] = thetaminus[i][0]- epsilon                              # Step 2        \n",
    "        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                 # Step 3\n",
    "        ### END CODE HERE ###\n",
    "        #parameters_values是将parameters字典平展开后的一个列向量，thetaminus复制了一遍作为θ参数，θ+就是列向量的每一个元素都加上一个很小的值\n",
    "        #得到一个新的向量，将这个新的向量再逆转回一个矩阵作为新的parameters字典参数，使用前向传播得出此时的代价\n",
    "        #以上使用的for循环实现，i表示这个列向量总共有多少个数，每次循环都首先把θ向量置为由parameters字典转化得到的列向量，\n",
    "        #每次循环依次修改其中一个元素，通过一个元素值的改变，使得权重矩阵那些参数的parameters字典改变了，\n",
    "        #所以每次迭代都能计算出一个不同的代价cost值，通过循环一共那么多元素次，得到一个J_plus列向量和一个J_minus列向量，将两个一减就得到近似梯度值向量，\n",
    "        #但是代码里面不使用向量减的，而是直接每次计算出向量的对应元素做减法\n",
    "        \n",
    "        # Compute gradapprox[i]\n",
    "        ### START CODE HERE ### (approx. 1 line)\n",
    "        gradapprox[i] = (J_plus[i] - J_minus[i])/(2*epsilon )\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "    ### START CODE HERE ### (approx. 1 line)\n",
    "    numerator = np.linalg.norm(grad-gradapprox,ord=2)           # Step 1'\n",
    "    denominator = np.linalg.norm(grad,ord=2)+np.linalg.norm(gradapprox,ord=2)        # Step 2'\n",
    "    difference = numerator/ denominator                                       # Step 3'\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    if difference > 1e-7:\n",
    "        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    \n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mThere is a mistake in the backward propagation! difference = 1.18904178788e-07\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "X, Y, parameters = gradient_check_n_test_case()\n",
    "\n",
    "cost, cache = forward_propagation_n(X, Y, parameters)\n",
    "gradients = backward_propagation_n(X, Y, cache)\n",
    "difference = gradient_check_n(parameters, gradients, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看来，在backward_propagation_n代码中存在错误！很好，你已经实现了梯度检查。回到backward_propagation试图找到/纠正错误（提示：检查dW2和db1）。当您认为已修复时，重新运行渐变检查。记住，你需要重新执行单元定义backward_propagation_n()如果你修改代码。\n",
    "\n",
    "笔记：\n",
    "1. 梯度下降非常慢，使用$\\frac{\\partial J}{\\partial \\theta} \\approx  \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon}$ 计算梯度近似值计算的代价非常大，因为这个原因，我们不会在训练时候每一次迭代都计算梯度检验，如果梯度值是正确的，就只需要一点时间来检验。\n",
    "2. 梯度检验，至少正如我们所展示的那样，对dropout不起作用。你通常会运行梯度校验算法并且不同时使用dropout来确保你的BP是正确的，然后再添加dropout的。\n",
    "\n",
    "这一节学到什么：\n",
    "1. 梯度检验核对了 数值梯度和近似提速之间的相似性，即通过求导公式计算的，和根据导数定义计算的梯度值的差值不是很大。\n",
    "2. 梯度下降非常的慢，所以不会在训练的每次迭代都运行，你只需要在需要确保梯度值是正确的时候运行一下，然后就观点，并且使用反向传播作为实际的学习过程。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
