{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 建立你的深度神经网络：step by step\n",
    "   微专业第一门课神经网络第四章的第二个作业，构建你自己的神经网络。\n",
    "   在这之前，你已经训练过两层神经网络（即只有单隐藏层的神经网络），这次作业，你将建立一个深层神经网络。\n",
    "   在这节中，你将实现所有建立深度神经网络必须实现的方法，并且把这些方法运用到下一个作业week4_03中，用于图像识别。\n",
    "    \n",
    "在这次编程练习之后，你将学会：\n",
    "1. 使用非线性单元比如ReLU来实现你的模型\n",
    "2. 建立一个更深一点的神经网络（大于一个隐藏层）\n",
    "3. 实现一个简单好用的神经网络class\n",
    "注释：\n",
    "上标[l]表示这是关于神经网络第l层的数\n",
    "上标（i）表示这是第几个训练样本\n",
    "下脚标i表示这个某一层的第i个隐藏单元\n",
    "\n",
    "## 1 packages\n",
    "首先引进这些包，numpy，matplotlib已经说过了\n",
    "dnn_utils 提供了一些必要的方法后面会用到的\n",
    "testCases提供了一些测试用例来检测你的方法是否实现正确\n",
    "np.random.seed(1)是用来保证所有的随机函数调用的一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v2 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 任务大纲\n",
    "为了建立你的神经网络，你将实现几个“helper functions”，这些辅助函数将会用到下一个任务中去建立一个两层神经网络和一个L层神经网络。\n",
    "outline of assignment：\n",
    "1. 初始化一个双层神经网络的参数以及一个L层神经网络的参数\n",
    "2. 实现正向传播模块：\n",
    "    （1）完成正向传播的线性部分（得到Z[l]结果）\n",
    "    （2）使用激活函数（ReLU、sigmoid）\n",
    "    （3）将前面两步结合起来（Linear->Activation）正向函数\n",
    "    （4）将前面L-1层（Linear->ReLU）重复L-1次，再加上（Linear->Sigmoid）在最后一层，这些会得到一个新的L_model_forward函数\n",
    "3. 计算损失\n",
    "4. 实现反向传播过程\n",
    "     （1）计算一层的反向传播Linear部分\n",
    "     （2）加载的工具包里面会提供关于ReLU以及sigmoid的导数计算方法\n",
    "     （3）结合前面两步（Linear->Activation）\n",
    "     （4）前面L-1层使用的ReLU函数，最后一层为sigmoid，建立一个新的L_model_backward函数\n",
    "5. 更新参数\n",
    "\n",
    "注意：对于每一个正向传播的过程，都会有一个对应的反向传播过程，这也是为什么在每一步正向传播模块中需要把一些值存储在cache中。这些cache中的值在反向传播过程中计算梯度非常重要。\n",
    "\n",
    "## 3 初始化\n",
    "接下来要写两个辅助函数来初始化参数，第一个函数用来初始化双层神经网络的参数，另一个用来生成L层神经网络的初始化过程。\n",
    "### 3.1 双层神经网络\n",
    "练习：创建并且初始化双层神经网络的参数\n",
    "提示：使用random初始化权重矩阵。使用np.random.randn(shape)\\*0.01即W的初始化，关于参数b，使用0来初始化 np.zeros(shape)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756]\n",
      " [-0.00528172 -0.01072969]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[ 0.00865408 -0.02301539]]\n",
      "b2 = [[ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(2,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 L层神经网络\n",
    "对深度L层神经网络的初始化要比双层神经网络参数初始化复杂的多，因为有更多的权重矩阵以及b。当你实现 initialize_parameters_deep，你需要确保每一层的矩阵的维度。\n",
    "![深层神经网络每层参数维度例子](http://upload-images.jianshu.io/upload_images/5355764-afc79376df8cbaac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "提示：\n",
    "1. 这里深度神经网络前L-1层的激活函数使用的ReLU函数，最后一层使用的是sigmoid\n",
    "2. 我们需要把每一层的隐藏单元数量存在一个变量layer_dims里面，根据这个列表，我们更方便按照规则初始化参数w和b。\n",
    "3. 下面是L=1(单层神经网络)的实现，帮助你去实现一般情况。\n",
    "```python\n",
    "if L == 1:\n",
    "      parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "      parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 正向传播模块\n",
    "### 4.1 Linear Forward\n",
    "现在你已经实现了初始化参数，接下来将实现正向传播模块。\n",
    "![正向传播实现思路](http://upload-images.jianshu.io/upload_images/5355764-7ea43cf755c10ca4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "练习：实现正向传播过程中的线性计算部分\n",
    "提示：计算的数学公式为Z[l]=W[l]A[l-1]+b[l]，你会发现np.dot同样很有用，如果你的维度不匹配，打印出W.shape也可以检查。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意对上面这一步的理解，这一步只是实现了某一层里面关于线性计算的部分，实现的功能则是输入一个W,b参数以及计算出来的上一层激活值，最后实现计算z，并且在计算z之后把当前这一层的w b以及A都放进cache缓存里面，返回值则为计算出来的Z值以及cache。这里并没有考虑整个神经网络的L层，把一般的能涵盖整个神经网络的线性计算表示出来，因为是针对一层的通用公式，后面再考虑使用循环把整个L层的正向传播都计算出来。\n",
    "\n",
    "### 4.2 线性计算--激活值计算 正向\n",
    "![正向传播用到的两个激活函数](http://upload-images.jianshu.io/upload_images/5355764-f45dce5bb8843263.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "上面提到了sigmoid函数以及relu激活函数，我们已经实现好了在工具包里面，直接使用就行，他们都返回了计算出来的A值，以及新的一个叫做activation_cache的量，里面存放了Z。\n",
    "根据下面的单隐藏层神经网络梯度下降求法，我们将明白在反向传播过程到底需要哪些值，将它们存放到cache里面。\n",
    "![单隐藏层神经网络梯度下降导数求法](http://upload-images.jianshu.io/upload_images/5355764-e81ab797f0c4887b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "\n",
    "为了方便，你将把计算线性Z值和计算激活值A的函数组合在一起，实现（Linear->activation）的过程，因此，你要实现一个函数，计算Z并且计算A。\n",
    "![数学表达式](http://upload-images.jianshu.io/upload_images/5355764-6af57f8b214cee7d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache =  linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[ 0.96890023  0.11013289]]\n",
      "With ReLU: A = [[ 3.43896131  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 L层模型\n",
    "为了更方便的实现L层神经网络，我们需要一个函数去重复实现linear_activation_forward with RELU L-1次，再跟着实现 linear_activation_forward with SIGMOID一次。\n",
    "练习：根据上面的模型实现正向传播。\n",
    "提示：下面的代码中，AL将用来表示A上标[L]，并且A[L]=g(Z[L])=g(W[L]A[L-1]+b[L]),最后的A[L]即AL同样也称为y_hat，即最后预测的值。\n",
    "步骤：\n",
    "    1. 使用上面你已经写好的函数\n",
    "    2. 使用一个Loop循环来实现前面的ReLU应用L-1次\n",
    "    3. 不要忘记跟踪cache列表里面的内容，即把每次一循环得到的结果存放进一个大的cache列表里面，使用列表追加元素 list.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)],parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[ 0.17007265  0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!现在你已经有一个全面的正向传播了，输入为X，输出为A[L]包括了你所有训练样本的预测结果。同样也把所有的中间值都记录到了caches里面，使用A[L],你就可以计算损失值啦。\n",
    "\n",
    "## 5 代价函数\n",
    "你需要计算损失，因为你要检测你的模型是否在学习，使得损失在减小。\n",
    "练习：计算cross-entropy cost J,使用下面这个公式。\n",
    "![深度神经网络代价函数计算公式](http://upload-images.jianshu.io/upload_images/5355764-16017ad5fc2b260e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = - np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))/m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.414931599615\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 反向传播模块\n",
    "就像正向传播一样，我们需要实现几个辅助函数很好理清思路。记住反向传播视用来计算代价函数对于每个参数的梯度的。\n",
    "提示：和正向传播过程一样，我们要建立反向传播也分解成三个步骤。\n",
    "1. 线性backward\n",
    "2. Linear->activation backward，因为我们的激活函数有两种不同的\n",
    "3. [LINEAR -> RELU] × (L-1) -> LINEAR -> SIGMOID backward (whole model)\n",
    "\n",
    "### 6.1 Linear backward\n",
    "对于第l层，线性部分是计算z值，Z[l]=W[l]A[l-1]+b[l],接下来是使用激活函数计算。假设你已经计算得到了dZ[l]=偏L/偏Z[l],意思即是已经知道了Z的偏导，那么来计算关于参数W[l],b[l],A[l-1]的偏导数，根据链式法则。\n",
    "![某一层已知dZ[l]计算其他参数导数的公式](http://upload-images.jianshu.io/upload_images/5355764-7bd0bd7682a2a8b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "练习：使用上面的三个公式来计算线性部分的参数的偏导数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[ 0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Linear-activation backward\n",
    "接下来，你将创建一个函数把两个辅助函数整合在一起，则是在线性部分计算导数的以及激活函数也计算导数的，最后得到linear_activation_backward。\n",
    "\n",
    "为了帮助你实现linear_activation_backward，在工具包里面直接提供了两种激活函数的导数计算的函数。\n",
    "\n",
    "![激活函数求导辅助函数内容](http://upload-images.jianshu.io/upload_images/5355764-a30d4730b7ab92a3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "我们可以发现，要想计算当前层dZ[l],那么就得通过A=g(Z)来求，根据求导链式法则，需要的参数有dA[l]以及g(Z[l])导。\n",
    "在工具包直接提供的辅助函数里面，传入的参数为dA，以及activation_cache，因为这个缓存里面存放了当前层计算出来的Z[l]值。\n",
    "\n",
    "练习：实现反向传播Linear->activation过程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 L层反向传播\n",
    "现在你将要实现整个L层神经网络的反向传播函数。回想一下，当你实现 L_model_forward时如何完成的，在每一次迭代，你会存下一个cache，里面包含了（X，W，b，以及Z值），在反向传播魔铠中，我们将使用到这些变量去计算梯度。因此，在L层反向传播模块L_model_backward函数中，你将对多有隐藏层进行迭代，从最后一层L层开始。每一迭代过程中，你将使用cache中第l层存好的值来计算。\n",
    "\n",
    "初始化反向传播：在正向传播过程中，我们知道初始值A0其实就是X，那么同样的，反向过程，我们要知道初始值是什么，那么就应该是最后一层的dA[L]偏导，如何计算呢，使用下面的公式：\n",
    "![深度神经网络反向传播初始值dA[L]计算公式](http://upload-images.jianshu.io/upload_images/5355764-2fc48533f290d83a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "推导过程，如何得到这个结果的呢，首先考虑一个样本，那么对于J代价函数对A求导，计算结果如下：\n",
    "![代价函数对最后一层A求导](http://upload-images.jianshu.io/upload_images/5355764-0bd98d2ac89d538e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)\n",
    "所以要使用向量化方法同时对m个样本进行计算，其计算方法就是把每个样本的导数值计算起来，横向堆叠，所以向量化操作，使用np.divide()方法即可。\n",
    "\n",
    "接下来的步骤，就是使用dA[L]放进LINEAR->SIGMOID backward函数，再使用for循环对前面的L-1层使用Linear->relu的梯度计算。这些过程中用到的参数都是从正向传播存好的cache中取得的。同样的，为了后面迭代很多次对参数进行更新，反向传播计算出来的梯度值，我们也需要存储起来，因此将dA，dW，db存放到grads字典里面。参数的不同层标记我们都是大写字母后面加个数字直接存放的，所以例子如下：**grads[\"dW\"+str(l)]=dW[l] **\n",
    "不像正向传播，对于超参数等的存放都一层作为一个单位，形成很多个cache，再放到caches列表里面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp =  linear_activation_backward(grads[\"dA\"+str(l+2)], current_cache, \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.          0.52257901]\n",
      " [ 0.         -0.3269206 ]\n",
      " [ 0.         -0.32070404]\n",
      " [ 0.         -0.74079187]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 更新参数\n",
    "在这一节中你将要使用计算出来的梯度来更新参数\n",
    "1. W[l]=W[l]−α dW[l] \n",
    "2. b[l]=b[l]−α db[l] \n",
    "\n",
    "并且为了循环迭代更新，更新好的参数仍然要存放在parameters字典里面\n",
    "练习：实现update_parameters()来使用梯度下降法更新你的参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 总结\n",
    "已经实现了建立一个深度神经网络需要的所有函数，我们知道这是一个很长的任务，坚持完成下去，将会变得更好。\n",
    "在下一节任务week4_03中，我们将会把上面的融合在一起建立两种模型：\n",
    "1. 双层神经网络模型\n",
    "2. L层神经网络模型\n",
    "事实上你会使用这些模型来对猫图和非猫图进行分类。\n",
    "\n",
    "暂时还没有实际应用到数据集上，但是已经完成了实现L层神经网络的整个过程需要的函数，对于这节任务，收获还是很多，其中最主要的就是如何将理论转化成代码，如何变成数字符号，让电脑去运作。而且理论部分，我们提到了正向和反向传播过程需要的参数，需要存放进cache存储，已经两个过程的初始值，在完成这节任务前，我都觉得很难用代码实现，写成代码的逻辑会不清楚，总结的说，以下几点很重要：\n",
    "1. 分解过程，将正向传播的L层实现，分三步完成，首先第一步计算z，第二步，将计算z和计算A结合（在这里也就实现了对两种不同激活函数的不同运算，第三步，对L层的前L-1层使用循环计算，最后一层加上sigmoid的运算。这样分解步骤的好处在于，层层递进，为下一步提供参数，并且由于我们知道后面反向传播会用到前面计算出来的值，在分解的过程我们就对参数值进行了存储，在字典或者元祖里面，很巧妙。当然，对于反向传播过程的分解，也同理相似，反向传播过程则是前面的cache作为参数，输出就相对简单一些，只需要把梯度值存起来放到grads字典里。\n",
    "2. 关于变量类型选择，对于参数W，b,因为要使用的比较多，所以采用字母后面加数字来表示每一层，全都存放在一个parameters字典里，而对于其余的超参数，例如，A，W，b作为一组，计算z放在一个linear_cache里面，再加上激活函数，计算过后，把Z也和前面三个参数一起存放到另一个linear_activation_cache字典里。因为后面L层实现，对每一层都要起作用，全放在一个字典修改也比较麻烦，因此将每一层的所有cache作为一个整理，每迭代一次一层，就把这一层的cache都放进一个caches列表里面，在for循环每一层时，再根据下角标依次取出来。又比如说，对于层数，我们要进行循环，因此使用列表，将每一个隐藏层的节点数存在列表里，对列表长度进行循环每一层，取出每层节点数也简单方便。\n",
    "\n",
    "总之，关于深度神经网络的实现就是一个复杂的过程，通过逐个函数实现，明确每个函数功能，最后再整合，可能把问题简单化，让每个函数实现的功能尽量单一，最后将小函数合成大函数，也就完成我们的模型的建立。"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
